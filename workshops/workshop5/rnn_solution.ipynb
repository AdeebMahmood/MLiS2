{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhR+NQ3p6XOPvT0/3xv/M2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/workshops/workshop5/rnn_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeUfbxv9ghmQ",
        "colab_type": "text"
      },
      "source": [
        "In lectures we hand-coded the BPTT algorithm to train an RNN language model to predict the next word in a sentence.\n",
        "\n",
        "Using the same training corpus, train a many-to-many LSTM model using TF2 to perform the same task, and compare your results against a vanilla RNN.\n",
        "\n",
        "In this example we concatenate all the sentences into a single vector to make it easier to feed into the TF2 dataset API. We therefore only have a single stop word between sentences.\n",
        "\n",
        "The downside of this approach is that sentences in different reviews will have different context, so ideally we would treat different reviews separately and pad inputs where necessary. \n",
        "\n",
        "Tensorflow has a similar character level RNN (not at the word level) here to help you: https://www.tensorflow.org/tutorials/text/text_generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4VLQKu0ofFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alq1W_bCWoTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDPfRGRmpHPN",
        "colab_type": "code",
        "outputId": "c65c0dac-4ff4-426c-8892-dc1a54dc15c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhl7G_SQ7wlH",
        "colab_type": "text"
      },
      "source": [
        "Download NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRyc8euxWpzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBvLWBQt8EZ6",
        "colab_type": "text"
      },
      "source": [
        "Upload imdb_sentences.txt file (or another file containing a list of sentences if you wish)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfpGBFRj-DzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isfile('imdb_sentences.txt'):\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHNYmTqB93G9",
        "colab_type": "text"
      },
      "source": [
        "Add sentence start and end tags, convert to lower case and strip newlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyBM3rWv9chG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_start_token = \"SENTENCE_STOP\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8QVY1IkAYEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('imdb_sentences.txt', 'r') as f:\n",
        "  sentences = f.readlines()\n",
        "sentences = [\"%s %s\" % (sentence_start_token, x.lstrip().rstrip('.\\n').lower()) for x in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPIWW5lqtSLG",
        "colab_type": "code",
        "outputId": "ec2a3070-95aa-4e48-e03d-1107b05651e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
        "for i in range(0, 10):\n",
        "  print(\"Example: %s\" % sentences[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsed 12188 sentences.\n",
            "Example: SENTENCE_STOP story of a man who has unnatural feelings for a pig\n",
            "Example: SENTENCE_STOP starts out with a opening scene that is a terrific example of absurd comedy\n",
            "Example: SENTENCE_STOP a formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers\n",
            "Example: SENTENCE_STOP unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting\n",
            "Example: SENTENCE_STOP even those from the era should be turned off\n",
            "Example: SENTENCE_STOP the cryptic dialogue would make shakespeare seem easy to a third grader\n",
            "Example: SENTENCE_STOP on a technical level it's better than you might think with some good cinematography by future great vilmos zsigmond\n",
            "Example: SENTENCE_STOP future stars sally kirkland and frederic forrest can be seen briefly\n",
            "Example: SENTENCE_STOP airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman philip stevens (james stewart) who is flying them & a bunch of vip's to his estate in preparation of it being opened to the public as a museum, also on board is stevens daughter julie (kathleen quinlan) & her son\n",
            "Example: SENTENCE_STOP the luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot chambers (robert foxworth) & his two accomplice's banker (monte markham) & wilson (michael pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent chambers almost hits an oil rig in the ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the bermuda triangle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTBDO_fs-udT",
        "colab_type": "text"
      },
      "source": [
        "Tokenize the sentences into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRfoIQZV-tNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDJ1NGJy-7i_",
        "colab_type": "code",
        "outputId": "e2d0c084-3f7c-4913-f8d6-033b97b7ffbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18153 unique words tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGpHm2s_IyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 1000\n",
        "unknown_token = 'UNKNOWN_TOKEN'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBTJmo8G_Fv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = word_freq.most_common(vocab_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "index_to_word = np.array(index_to_word)\n",
        "word_to_index = dict([(w,i) for i, w in enumerate(index_to_word)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-C-wm5h_kQB",
        "colab_type": "text"
      },
      "source": [
        "Replace all words not in our vocabulary with the unknown token and discard sentences under min / over max number of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBo7x_Zpd77V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_sentence_length = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhY2tX9duHzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purged_sentences = []\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "  if len(sent) >= min_sentence_length:\n",
        "    purged_sentences.append([w if w in word_to_index else unknown_token for w in sent])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHynO1W2x99x",
        "colab_type": "text"
      },
      "source": [
        "Flatten sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXT2EKyw_h6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = [word for sent in purged_sentences for word in sent]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMdDdrLlyBEx",
        "colab_type": "text"
      },
      "source": [
        "Convert to integer representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r09ZMRxqk7pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_as_int = np.array([word_to_index[w] for w in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ7O4LNYyS5U",
        "colab_type": "text"
      },
      "source": [
        "Set maximum length sentence we want for a single input in characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDZy4RUuyOUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx-hXzf4_8g1",
        "colab_type": "text"
      },
      "source": [
        "Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmy5ssHlwbfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDM9GOCzwuix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3NDYIojyJ5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XszttQKyyn3T",
        "colab_type": "code",
        "outputId": "d8a10d86-3f48-48b0-a66b-7dab9917fd2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(' '.join(index_to_word[input_example.numpy()])))\n",
        "  print ('Target data:', repr(' '.join(index_to_word[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"SENTENCE_STOP story of a man who has UNKNOWN_TOKEN UNKNOWN_TOKEN for a UNKNOWN_TOKEN SENTENCE_STOP starts out with a opening scene that is a UNKNOWN_TOKEN example of absurd comedy SENTENCE_STOP a UNKNOWN_TOKEN UNKNOWN_TOKEN audience is turned into an UNKNOWN_TOKEN , UNKNOWN_TOKEN UNKNOWN_TOKEN by the crazy UNKNOWN_TOKEN of it 's UNKNOWN_TOKEN SENTENCE_STOP unfortunately it UNKNOWN_TOKEN absurd the whole time with no general UNKNOWN_TOKEN eventually making it just too off UNKNOWN_TOKEN SENTENCE_STOP even those from the UNKNOWN_TOKEN should be turned off SENTENCE_STOP the UNKNOWN_TOKEN dialogue would make UNKNOWN_TOKEN seem UNKNOWN_TOKEN to a third UNKNOWN_TOKEN SENTENCE_STOP on a UNKNOWN_TOKEN level it 's better than you\"\n",
            "Target data: \"story of a man who has UNKNOWN_TOKEN UNKNOWN_TOKEN for a UNKNOWN_TOKEN SENTENCE_STOP starts out with a opening scene that is a UNKNOWN_TOKEN example of absurd comedy SENTENCE_STOP a UNKNOWN_TOKEN UNKNOWN_TOKEN audience is turned into an UNKNOWN_TOKEN , UNKNOWN_TOKEN UNKNOWN_TOKEN by the crazy UNKNOWN_TOKEN of it 's UNKNOWN_TOKEN SENTENCE_STOP unfortunately it UNKNOWN_TOKEN absurd the whole time with no general UNKNOWN_TOKEN eventually making it just too off UNKNOWN_TOKEN SENTENCE_STOP even those from the UNKNOWN_TOKEN should be turned off SENTENCE_STOP the UNKNOWN_TOKEN dialogue would make UNKNOWN_TOKEN seem UNKNOWN_TOKEN to a third UNKNOWN_TOKEN SENTENCE_STOP on a UNKNOWN_TOKEN level it 's better than you might\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_I6PqN7yrAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nuPkFZr0-Ds",
        "colab_type": "text"
      },
      "source": [
        "**Now code and train your RNN...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_E9upwd008X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dry2o5xHj9wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihb6AmTdeReg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim = embedding_dim,\n",
        "  rnn_units = rnn_units,\n",
        "  batch_size = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9hWVxGnj5WH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3OqkVO9kU_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ-GT4bBoYKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQq234H_kYV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRwVXw8zkdZF",
        "colab_type": "code",
        "outputId": "b75086d8-d41a-491b-ee5c-f16fe5f38422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 44 steps\n",
            "Epoch 1/50\n",
            "44/44 [==============================] - 6s 138ms/step - loss: 5.0517\n",
            "Epoch 2/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 4.7332\n",
            "Epoch 3/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 4.4881\n",
            "Epoch 4/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 4.2619\n",
            "Epoch 5/50\n",
            "44/44 [==============================] - 5s 120ms/step - loss: 4.1334\n",
            "Epoch 6/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 4.0488\n",
            "Epoch 7/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 3.9779\n",
            "Epoch 8/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 3.9122\n",
            "Epoch 9/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 3.8559\n",
            "Epoch 10/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 3.8021\n",
            "Epoch 11/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 3.7551\n",
            "Epoch 12/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 3.7096\n",
            "Epoch 13/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 3.6652\n",
            "Epoch 14/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 3.6245\n",
            "Epoch 15/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 3.5781\n",
            "Epoch 16/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 3.5323\n",
            "Epoch 17/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 3.4884\n",
            "Epoch 18/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 3.4428\n",
            "Epoch 19/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 3.3965\n",
            "Epoch 20/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 3.3473\n",
            "Epoch 21/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 3.2924\n",
            "Epoch 22/50\n",
            "44/44 [==============================] - 5s 120ms/step - loss: 3.2389\n",
            "Epoch 23/50\n",
            "44/44 [==============================] - 5s 119ms/step - loss: 3.1836\n",
            "Epoch 24/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 3.1255\n",
            "Epoch 25/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 3.0638\n",
            "Epoch 26/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 3.0003\n",
            "Epoch 27/50\n",
            "44/44 [==============================] - 5s 119ms/step - loss: 2.9356\n",
            "Epoch 28/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 2.8673\n",
            "Epoch 29/50\n",
            "44/44 [==============================] - 5s 116ms/step - loss: 2.7960\n",
            "Epoch 30/50\n",
            "44/44 [==============================] - 5s 119ms/step - loss: 2.7244\n",
            "Epoch 31/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 2.6553\n",
            "Epoch 32/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 2.5844\n",
            "Epoch 33/50\n",
            "44/44 [==============================] - 5s 119ms/step - loss: 2.5102\n",
            "Epoch 34/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 2.4369\n",
            "Epoch 35/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 2.3640\n",
            "Epoch 36/50\n",
            "44/44 [==============================] - 5s 115ms/step - loss: 2.2956\n",
            "Epoch 37/50\n",
            "44/44 [==============================] - 5s 119ms/step - loss: 2.2227\n",
            "Epoch 38/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 2.1532\n",
            "Epoch 39/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 2.0832\n",
            "Epoch 40/50\n",
            "44/44 [==============================] - 5s 121ms/step - loss: 2.0156\n",
            "Epoch 41/50\n",
            "44/44 [==============================] - 5s 119ms/step - loss: 1.9539\n",
            "Epoch 42/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 1.8904\n",
            "Epoch 43/50\n",
            "44/44 [==============================] - 5s 120ms/step - loss: 1.8258\n",
            "Epoch 44/50\n",
            "44/44 [==============================] - 5s 119ms/step - loss: 1.7664\n",
            "Epoch 45/50\n",
            "44/44 [==============================] - 5s 117ms/step - loss: 1.7031\n",
            "Epoch 46/50\n",
            "44/44 [==============================] - 5s 120ms/step - loss: 1.6475\n",
            "Epoch 47/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 1.5930\n",
            "Epoch 48/50\n",
            "44/44 [==============================] - 5s 119ms/step - loss: 1.5368\n",
            "Epoch 49/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 1.4874\n",
            "Epoch 50/50\n",
            "44/44 [==============================] - 5s 118ms/step - loss: 1.4394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivypVshhkhl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkKiKWEiqNRE",
        "colab_type": "code",
        "outputId": "de13a24c-23dc-4ff0-f9e1-2cef1c8a1beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8d00be3978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP4MclNSqRiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5NXlrW7qT_H",
        "colab_type": "code",
        "outputId": "609f5c6a-9a03-4899-f3a0-8f83f7616c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            256000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 1000)           1025000   \n",
            "=================================================================\n",
            "Total params: 6,527,976\n",
            "Trainable params: 6,527,976\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-aIFyZ_sag6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unknown_index = word_to_index['UNKNOWN_TOKEN']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcvcq1VVqWBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_word):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 30\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [word_to_index[start_word]]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      if predicted_id != unknown_index:\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index_to_word[predicted_id])\n",
        "\n",
        "  sentence = ' '.join(text_generated)\n",
        "  sentence = sentence.replace('SENTENCE_STOP', '. ')\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh2FTk-gqbe9",
        "colab_type": "code",
        "outputId": "21b302fd-5a76-4ac3-a998-72612d65a997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for i in range(5):\n",
        "  print(generate_text(model, start_word=u\"SENTENCE_STOP\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "since version years ago , one new need to be seen from a good .  what a funny movie is when i believe the role was bad\n",
            "whether 3 songs are fine , the production ... you so far the best thing to watch this film than going to save you more and\n",
            "goes at times simply also found it that the most different things were on this movie to be evil , the dialog 's\n",
            "bored of all is awful .  i ca n't blame the producers who needs to be talent and as well as the\n",
            "under is happy with the most of these episodes throughout .  these were so , i mean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACKmHh7rqfGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}