{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop1_question4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIO926UhBrWYOTZiIBBBaG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/workshops/workshop1/question4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tbHMKVAmIkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1GtAJWGmPZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0_VlZPamRNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2fd4f895-f7e3-4370-a52b-31890f7f47a0"
      },
      "source": [
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "print(X)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNgXlZK1mS4M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9f474cd8-d6af-4fe3-a2aa-e4c19aecda09"
      },
      "source": [
        "Y = np.array([[0], [1], [1], [0]])\n",
        "print(Y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdufl_ArmUdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    # Initialise with random weights\n",
        "    self.weights_1 = 0.1 * np.random.normal(size=(3,2))\n",
        "    self.weights_2 = 0.1 * np.random.normal(size=(3,2))\n",
        "    self.weights_3 = 0.1 * np.random.normal(size=(3,1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Do a forward pass\n",
        "    if len(x.shape) == 1:\n",
        "      # Single example, so add a batch dimension of 1\n",
        "      x = np.expand_dims(x, axis=0)\n",
        "    # First hidden layer \n",
        "    z_1 = np.matmul(np.hstack((np.ones(shape=(x.shape[0], 1)), x)), self.weights_1)\n",
        "    # Apply ReLU activation function\n",
        "    a_1 = np.maximum(z_1, 0)\n",
        "    # Second hidden layer \n",
        "    z_2 = np.matmul(np.hstack((np.ones(shape=(a_1.shape[0], 1)), a_1)), self.weights_2)\n",
        "    # Apply ReLU activation function\n",
        "    a_2 = np.maximum(z_2, 0)\n",
        "    # Output layer\n",
        "    z_3 = np.matmul(np.hstack((np.ones(shape=(a_2.shape[0], 1)), a_2)), self.weights_3)\n",
        "    # Linear activation \n",
        "    a_3 = z_3\n",
        "    return z_1, a_1, z_2, a_2, z_3, a_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8nJVTAEmp17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 1\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI1drvnJmsdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "335d9103-ba40-4bab-8424-1f874ae56163"
      },
      "source": [
        "m = MLP()\n",
        "loss_history = []\n",
        "weights_1_history = []\n",
        "weights_2_history = []\n",
        "for epoch in range(num_epochs):\n",
        "  # Do forward pass\n",
        "  z_1, a_1, z_2, a_2, z_3, a_3 = m.forward(X)\n",
        "  loss = 0.25 * np.sum((a_2 - Y)**2)\n",
        "  loss_history.append(loss)\n",
        "  if epoch % 100 == 0:\n",
        "    print(epoch, loss)\n",
        "  # Delta_2 has shape(4, 1), the first dimension being the batch dimension\n",
        "  delta_3 = 0.5 * ( a_3 - Y)\n",
        "  g_prime_2 = np.heaviside(z_2, 0)\n",
        "  # Delta_1 has shape (4, 2)\n",
        "  delta_2 = np.matmul(delta_3, m.weights_3[1:3, :].T) * g_prime_2 \n",
        "  print(delta_2.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.9759331013626684\n",
            "(4, 1) (1, 2)\n",
            "(4, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p1Wgxalm2eE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}