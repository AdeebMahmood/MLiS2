{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitpytorchcondaa0b534c2b8494e44a9733e9c61af76bb",
   "display_name": "Python 3.7.6 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/reinforcement_learning/examples/reinforcement_learning/tabular/monte_carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining our environment as a class: we need this to have two methods which the agent can interact with, to induce transitions in the state, and to restart the environment after each episode, which we call step and reset respectively. Transition calls _reward, which provides the reward for the transition which just occured, and provides the new state, reward, and whether the episode has ended as output to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class excursions(object):\n",
    "\t\"\"\"A simple environment which provides rewards based on excursions.\"\"\"\n",
    "\n",
    "\tdef __init__(self, parameters):\n",
    "\t\tself.trajectory_length = parameters['trajectory_length']\n",
    "\t\tself.positivity_bias = parameters['positivity_bias']\n",
    "\t\tself.target_bias = parameters['target_bias']\n",
    "\t\tself.action = 0\n",
    "\t\tself.state = [0, 0]\n",
    "\t\tself.terminal_state = False\n",
    "\n",
    "\tdef _reward(self):\n",
    "\t\t\"\"\"Calculates the reward for the last transition to occur.\"\"\"\n",
    "\t\tif self.state[0] < 0:\n",
    "\t\t\treward = -self.positivity_bias * abs(self.state[0])\n",
    "\t\telse:\n",
    "\t\t\treward = 0\n",
    "\t\tif self.state[1] == self.trajectory_length:\n",
    "\t\t\treward -= self.target_bias * abs(self.state[0])\n",
    "\t\treturn reward\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\t\"\"\"Updates the environment state based on the input action.\"\"\"\n",
    "\t\tself.action = action\n",
    "\t\tself.state[0] += 2*action - 1\n",
    "\t\tself.state[1] += 1\n",
    "\t\tif self.state[1] == self.trajectory_length:\n",
    "\t\t\tself.terminal_state = True\n",
    "\t\treturn self.state, self._reward(), self.terminal_state\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\t\"\"\"Resets the environment state and terminal boolean.\"\"\"\n",
    "\t\tself.action = 0\n",
    "\t\tself.state = [0, 0]\n",
    "\t\tself.terminal_state = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need our table for the policy, for which we will need some functions from math and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numpy.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table will act as the agent, so it needs to be able to output a randomly selected action. The action function calls the forward function, which simply returns one of the two action probabilities for the current state.\n",
    "\n",
    "To learn, it needs to be able to update the probabilities, which is done using the step function. To facilitate this, the eligibility (the derivative of the log of the policy) is output as a second return from action, which the learning algorithm can then input to step when later required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_action_policy_table(object):\n",
    "\t\"\"\"A tabular policy for environments where each state has two actions.\"\"\"\n",
    "\n",
    "\tdef __init__(self, dimensions, learning_rate):\n",
    "\t\tself.table = np.zeros(dimensions)\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\t\"\"\"Calculates the probabilitiy of action 1.\"\"\"\n",
    "\t\texponentiated_potential = math.exp(-self.table[state[0]][state[1]])\n",
    "\t\treturn 1/(exponentiated_potential+1)\n",
    "\n",
    "\tdef action(self, state):\n",
    "\t\t\"\"\"Returns a random action according to the current policy.\"\"\"\n",
    "\t\taction1_probability = self.forward(state)\n",
    "\t\trandom = numpy.random.random() # pylint: disable = no-member\n",
    "\t\tif random < action1_probability:\n",
    "\t\t\treturn 1, 1 - action1_probability\n",
    "\t\telse:\n",
    "\t\t\treturn 0, -action1_probability\n",
    "\t\n",
    "\tdef step(self, state, error, eligibility):\n",
    "\t\t\"\"\"Updates the potential for actions in the given state.\"\"\"\n",
    "\t\tself.table[state[0]][state[1]] += self.learning_rate * error * eligibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to define our algorithm. The three algorithms we will consider in the tabular case share much of the same structure, so we can make use of inheritance to make the code more compact and highlight the distinctions between the three. As such, we begin by defining a base class for episodic RL algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class episodic_algorithm(object):\n",
    "\t\"\"\"A wrapper for episodic RL algorithms.\"\"\"\n",
    "\n",
    "\tdef __init__(self, parameters):\n",
    "\t\tself.environment = parameters['environment']\n",
    "\t\tself.average_return = 0\n",
    "\t\tself.average_returns = []\n",
    "\t\tself.returns = []\n",
    "\t\tself.return_learning_rate = parameters['return_learning_rate']\n",
    "\t\tself.policy = parameters['policy']\n",
    "\t\tself.episode = 0\n",
    "\t\tself.past_state = self.environment.state.copy()\n",
    "\t\tself.action = 0\n",
    "\t\tself.current_state = self.environment.state.copy()\n",
    "\t\tself.reward = 0\n",
    "\t\tself.current_return = 0\n",
    "\t\tself.terminal_state = False\n",
    "\t\n",
    "\tdef _transition(self):\n",
    "\t\tself.past_state = self.current_state.copy()\n",
    "\t\tself.action, self.eligibility = self.policy.action(self.current_state)\n",
    "\t\tself.current_state, self.reward, self.terminal_state = self.environment.step(\n",
    "\t\t\tself.action)\n",
    "\t\tself.current_return += self.reward\n",
    "\n",
    "\tdef _per_step(self):\n",
    "\t\tself._transition()\n",
    "\n",
    "\tdef _per_episode(self):\n",
    "\t\tself.environment.reset()\n",
    "\t\tself.past_state = self.environment.state.copy()\n",
    "\t\tself.current_state = self.environment.state.copy()\n",
    "\t\tself.terminal_state = False\n",
    "\n",
    "\tdef _episode(self):\n",
    "\t\tself.current_return = 0\n",
    "\t\twhile not self.terminal_state:\n",
    "\t\t\tself._per_step()\n",
    "\t\tself._per_episode()\n",
    "\t\tself.average_return += self.return_learning_rate * (self.current_return \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t- self.average_return)\n",
    "\t\tself.episode += 1\n",
    "\n",
    "\tdef train(self, episodes):\n",
    "\t\tself.episode = 0\n",
    "\t\twhile self.episode < episodes:\n",
    "\t\t\tself._episode()\n",
    "\t\t\tself.average_returns.append(self.average_return)\n",
    "\t\t\tself.returns.append(self.current_return)\n",
    "\n",
    "\tdef _sample(self):\n",
    "\t\ttrajectory = []\n",
    "\t\twhile not self.terminal_state:\n",
    "\t\t\tself._transition()\n",
    "\t\t\ttrajectory.append(self.current_state)\n",
    "\t\tself.environment.reset()\n",
    "\t\tself.past_state = self.environment.state.copy()\n",
    "\t\tself.current_state = self.environment.state.copy()\n",
    "\t\tself.terminal_state = False\n",
    "\t\treturn trajectory\n",
    "\n",
    "\tdef samples(self, sample_count):\n",
    "\t\ttrajectories = []\n",
    "\t\tsample = 0\n",
    "\t\twhile sample < sample_count:\n",
    "\t\t\ttrajectory = self._sample()\n",
    "\t\t\ttrajectories.append(trajectory)\n",
    "\t\t\tsample += 1\n",
    "\t\treturn trajectories\n",
    "\n",
    "\tdef _return_sample(self):\n",
    "\t\tself.current_return = 0\n",
    "\t\twhile not self.terminal_state:\n",
    "\t\t\tself._transition()\n",
    "\t\tself.environment.reset()\n",
    "\t\tself.past_state = self.environment.state.copy()\n",
    "\t\tself.current_state = self.environment.state.copy()\n",
    "\t\tself.terminal_state = False\n",
    "\n",
    "\tdef evaluate(self, sample_count, set_average = True):\n",
    "\t\tsample = 1\n",
    "\t\taverage_return = 0\n",
    "\t\twhile sample <= sample_count:\n",
    "\t\t\tself._return_sample()\n",
    "\t\t\taverage_return += (self.current_return - average_return)/sample\n",
    "\t\t\tsample += 1\n",
    "\t\tif set_average:\n",
    "\t\t\tself.average_return = average_return\n",
    "\t\treturn average_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of this base class we then define our first policy gradient algorithm, REINFORCE, or Monte Carlo returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monte_carlo_returns(episodic_algorithm):\n",
    "\t\"\"\"A purely return based policy gradient algorithm.\"\"\"\n",
    "\n",
    "\tdef __init__(self, parameters):\n",
    "\t\tsuper().__init__(parameters)\n",
    "\t\tself.states = []\n",
    "\t\tself.rewards = []\n",
    "\t\tself.eligibilities = []\n",
    "\n",
    "\tdef _per_step(self):\n",
    "\t\tself._transition()\n",
    "\t\tself.states.append(self.past_state)\n",
    "\t\tself.rewards.append(self.reward)\n",
    "\t\tself.eligibilities.append(self.eligibility)\n",
    "\n",
    "\tdef _update(self):\n",
    "\t\tself.rewards = np.array(self.rewards)\n",
    "\t\tfor index in range(len(self.states)):\n",
    "\t\t\tstate_return = np.sum(self.rewards[index:])\n",
    "\t\t\tself.policy.step(self.states[index], state_return, self.eligibilities[index])\n",
    "\n",
    "\tdef _per_episode(self):\n",
    "\t\tself._update()\n",
    "\t\tsuper()._per_episode()\n",
    "\t\tself.states = []\n",
    "\t\tself.rewards = []\n",
    "\t\tself.eligibilities = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this, we begin by generating samples and calculating the average return for the initial 50/50 action policy. We then train with the above algorithm, and repeat the process with the resulting policy."
   ]
  }
 ]
}