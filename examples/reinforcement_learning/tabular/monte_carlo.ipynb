{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitpytorchcondaa0b534c2b8494e44a9733e9c61af76bb",
   "display_name": "Python 3.7.6 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/reinforcement_learning/examples/reinforcement_learning/tabular/monte_carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining our environment as a class: we need this to have two methods which the agent can interact with, to induce transitions in the state, and to restart the environment after each episode, which we call transition and reset respectively. Transition calls _reward, which provides the reward for the transition which just occured, and provided the new state and reward as output to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class excursions(object):\n",
    "\n",
    "\tdef __init__(self, parameters):\n",
    "\t\tself.trajectory_length = parameters['trajectory_length']\n",
    "\t\tself.positivity_bias = parameters['positivity_bias']\n",
    "\t\tself.target_bias = parameters['target_bias']\n",
    "\t\tself.action = 0\n",
    "\t\tself.state = [0, 0]\n",
    "\t\tself.terminal_state = False\n",
    "\n",
    "\tdef _reward(self):\n",
    "\t\tif self.state[0] < 0:\n",
    "\t\t\treward = -self.positivity_bias * abs(self.state[0])\n",
    "\t\telse:\n",
    "\t\t\treward = 0\n",
    "\t\tif self.state[1] == self.trajectory_length:\n",
    "\t\t\treward -= self.target_bias * abs(self.state[0])\n",
    "\t\treturn reward\n",
    "\n",
    "\tdef transition(self, action):\n",
    "\t\tself.action = action\n",
    "\t\tself.state[0] += 2*action - 1\n",
    "\t\tself.state[1] += 1\n",
    "\t\tif self.state[1] == self.trajectory_length:\n",
    "\t\t\tself.terminal_state = True\n",
    "\t\treturn self.state, self._reward()\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.action = 0\n",
    "\t\tself.state = [0, 0]\n",
    "\t\tself.terminal_state = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}