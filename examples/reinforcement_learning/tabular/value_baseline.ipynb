{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitpytorchcondaa0b534c2b8494e44a9733e9c61af76bb",
   "display_name": "Python 3.7.6 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/reinforcement_learning/examples/reinforcement_learning/tabular/value_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this new algorithm, we will need to define two new objects: the value table, and the algorithm itself. We begin by pasting in the classes from the previous notebook, for reuse and comparison with the first algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"colab":{},"cellView":"form"},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
     "output_type": "error",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
      "at A.startServer (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:786120)",
      "at async A.ensureServerAndNotebookImpl (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:785575)"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "\n",
    "class excursions(object):\n",
    "\t\"\"\"A simple environment which provides rewards based on excursions.\"\"\"\n",
    "\n",
    "\tdef __init__(self, parameters):\n",
    "\t\tself.trajectory_length = parameters['trajectory_length']\n",
    "\t\tself.positivity_bias = parameters['positivity_bias']\n",
    "\t\tself.target_bias = parameters['target_bias']\n",
    "\t\tself.action = 0\n",
    "\t\tself.state = [0, 0]\n",
    "\t\tself.terminal_state = False\n",
    "\n",
    "\tdef _reward(self):\n",
    "\t\t\"\"\"Calculates the reward for the last transition to occur.\"\"\"\n",
    "\t\tif self.state[0] < 0:\n",
    "\t\t\treward = -self.positivity_bias * abs(self.state[0])\n",
    "\t\telse:\n",
    "\t\t\treward = 0\n",
    "\t\tif self.state[1] == self.trajectory_length:\n",
    "\t\t\treward -= self.target_bias * abs(self.state[0])\n",
    "\t\treturn reward\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\t\"\"\"Updates the environment state based on the input action.\"\"\"\n",
    "\t\tself.action = action\n",
    "\t\tself.state[0] += 2*action - 1\n",
    "\t\tself.state[1] += 1\n",
    "\t\tif self.state[1] == self.trajectory_length:\n",
    "\t\t\tself.terminal_state = True\n",
    "\t\treturn self.state, self._reward(), self.terminal_state\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\t\"\"\"Resets the environment state and terminal boolean.\"\"\"\n",
    "\t\tself.action = 0\n",
    "\t\tself.state = [0, 0]\n",
    "\t\tself.terminal_state = False\n",
    "\n",
    "\n",
    "class two_action_policy_table(object):\n",
    "\t\"\"\"A tabular policy for environments where each state has two actions.\"\"\"\n",
    "\n",
    "\tdef __init__(self, dimensions, learning_rate):\n",
    "\t\tself.table = np.zeros(dimensions)\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\n",
    "\tdef _forward(self, state):\n",
    "\t\t\"\"\"Calculates the probabilitiy of action 1.\"\"\"\n",
    "\t\texponentiated_potential = math.exp(-self.table[state[0]][state[1]])\n",
    "\t\treturn 1/(exponentiated_potential+1)\n",
    "\n",
    "\tdef action(self, state):\n",
    "\t\t\"\"\"Returns a random action according to the current policy.\"\"\"\n",
    "\t\taction1_probability = self._forward(state)\n",
    "\t\trandom = numpy.random.random()\n",
    "\t\tif random < action1_probability:\n",
    "\t\t\treturn 1, 1 - action1_probability\n",
    "\t\telse:\n",
    "\t\t\treturn 0, -action1_probability\n",
    "\t\n",
    "\tdef step(self, state, error, eligibility):\n",
    "\t\t\"\"\"Updates the potential for actions in the given state.\"\"\"\n",
    "\t\tself.table[state[0]][state[1]] += self.learning_rate * error * eligibility\n",
    "\n",
    "\n",
    "class episodic_algorithm(object):\n",
    "\t\"\"\"A wrapper for episodic RL algorithms.\"\"\"\n",
    "\n",
    "\tdef __init__(self, parameters):\n",
    "\t\tself.environment = parameters['environment']\n",
    "\t\tself.average_return = 0\n",
    "\t\tself.average_returns = []\n",
    "\t\tself.returns = []\n",
    "\t\tself.return_learning_rate = parameters['return_learning_rate']\n",
    "\t\tself.policy = parameters['policy']\n",
    "\t\tself.episode = 0\n",
    "\t\tself.past_state = self.environment.state.copy()\n",
    "\t\tself.action = 0\n",
    "\t\tself.current_state = self.environment.state.copy()\n",
    "\t\tself.reward = 0\n",
    "\t\tself.current_return = 0\n",
    "\t\tself.terminal_state = False\n",
    "\t\n",
    "\tdef _transition(self):\n",
    "\t\t\"\"\"Requests an action from the policy and sends it to the environment.\"\"\"\n",
    "\t\tself.past_state = self.current_state.copy()\n",
    "\t\tself.action, self.eligibility = self.policy.action(self.current_state)\n",
    "\t\tself.current_state, self.reward, self.terminal_state = self.environment.step(\n",
    "\t\t\tself.action)\n",
    "\t\tself.current_return += self.reward\n",
    "\n",
    "\tdef _per_step(self):\n",
    "\t\t\"\"\"A placeholder for a learning algorithms computations per transition.\"\"\"\n",
    "\t\tself._transition()\n",
    "\n",
    "\tdef _per_episode(self):\n",
    "\t\t\"\"\"A placeholder for a learning algorithms computations after episodes.\"\"\"\n",
    "\t\tself.environment.reset()\n",
    "\t\tself.past_state = self.environment.state.copy()\n",
    "\t\tself.current_state = self.environment.state.copy()\n",
    "\t\tself.terminal_state = False\n",
    "\n",
    "\tdef _episode(self):\n",
    "\t\t\"\"\"Uses _per_step and _per_episode to run a generic episodes computations.\"\"\"\n",
    "\t\tself.current_return = 0\n",
    "\t\twhile not self.terminal_state:\n",
    "\t\t\tself._per_step()\n",
    "\t\tself._per_episode()\n",
    "\t\tself.average_return += self.return_learning_rate * (self.current_return \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t- self.average_return)\n",
    "\t\tself.episode += 1\n",
    "\n",
    "\tdef train(self, episodes):\n",
    "\t\t\"\"\"Trains the policy by repeatedly running episodes, storing return info.\"\"\"\n",
    "\t\tself.episode = 0\n",
    "\t\twhile self.episode < episodes:\n",
    "\t\t\tself._episode()\n",
    "\t\t\tself.average_returns.append(self.average_return)\n",
    "\t\t\tself.returns.append(self.current_return)\n",
    "\n",
    "\tdef _sample(self):\n",
    "\t\t\"\"\"Generates a sample trajectory using the current policy.\"\"\"\n",
    "\t\tself.current_return = 0\n",
    "\t\ttrajectory = [self.current_state.copy()]\n",
    "\t\twhile not self.terminal_state:\n",
    "\t\t\tself._transition()\n",
    "\t\t\ttrajectory.append(self.current_state.copy())\n",
    "\t\tself.environment.reset()\n",
    "\t\tself.past_state = self.environment.state.copy()\n",
    "\t\tself.current_state = self.environment.state.copy()\n",
    "\t\tself.terminal_state = False\n",
    "\t\treturn trajectory\n",
    "\n",
    "\tdef samples(self, sample_count):\n",
    "\t\t\"\"\"Generates a set of trajectory samples.\"\"\"\n",
    "\t\ttrajectories = []\n",
    "\t\tsample = 0\n",
    "\t\twhile sample < sample_count:\n",
    "\t\t\ttrajectory = self._sample()\n",
    "\t\t\ttrajectories.append(trajectory)\n",
    "\t\t\tsample += 1\n",
    "\t\treturn trajectories\n",
    "\n",
    "\tdef _return_sample(self):\n",
    "\t\t\"\"\"Runs an episode to sample a return for evaulation.\"\"\"\n",
    "\t\tself.current_return = 0\n",
    "\t\twhile not self.terminal_state:\n",
    "\t\t\tself._transition()\n",
    "\t\tself.environment.reset()\n",
    "\t\tself.past_state = self.environment.state.copy()\n",
    "\t\tself.current_state = self.environment.state.copy()\n",
    "\t\tself.terminal_state = False\n",
    "\n",
    "\tdef evaluate(self, sample_count, set_average = True):\n",
    "\t\t\"\"\"Evaluates the policy by estimating the average return.\"\"\"\n",
    "\t\tsample = 1\n",
    "\t\taverage_return = 0\n",
    "\t\twhile sample <= sample_count:\n",
    "\t\t\tself._return_sample()\n",
    "\t\t\taverage_return += (self.current_return - average_return)/sample\n",
    "\t\t\tsample += 1\n",
    "\t\tif set_average:\n",
    "\t\t\tself.average_return = average_return\n",
    "\t\treturn average_return\n",
    "\n",
    "\n",
    "class monte_carlo_returns(episodic_algorithm):\n",
    "\t\"\"\"A purely return based policy gradient algorithm.\"\"\"\n",
    "\n",
    "\tdef __init__(self, parameters):\n",
    "\t\tsuper().__init__(parameters)\n",
    "\t\tself.states = []\n",
    "\t\tself.rewards = []\n",
    "\t\tself.eligibilities = []\n",
    "\n",
    "\tdef _per_step(self):\n",
    "\t\t\"\"\"Adds required data storage for learning post-episode.\"\"\"\n",
    "\t\tself._transition()\n",
    "\t\tself.states.append(self.past_state)\n",
    "\t\tself.rewards.append(self.reward)\n",
    "\t\tself.eligibilities.append(self.eligibility)\n",
    "\n",
    "\tdef _update(self):\n",
    "\t\t\"\"\"Loops over the episode in reverse, updating the policy in each state.\"\"\"\n",
    "\t\tself.rewards = np.array(self.rewards)\n",
    "\t\tstate_return = 0\n",
    "\t\tfor index in range(len(self.states) - 1, -1, -1):\n",
    "\t\t\tstate_return += self.rewards[index]\n",
    "\t\t\tself.policy.step(self.states[index], state_return, self.eligibilities[index])\n",
    "\n",
    "\tdef _per_episode(self):\n",
    "\t\t\"\"\"Adds additional resets relevant to learning algorithm.\"\"\"\n",
    "\t\tself._update()\n",
    "\t\tsuper()._per_episode()\n",
    "\t\tself.states = []\n",
    "\t\tself.rewards = []\n",
    "\t\tself.eligibilities = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the value table, we define essentially a stripped down version of the policy table: here, forward is called directly to access the value of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
     "output_type": "error",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
      "at A.startServer (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:786120)",
      "at async A.ensureServerAndNotebookImpl (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:785575)"
     ]
    }
   ],
   "source": [
    "class value_table(object):\n",
    "\t\"\"\"A generic tabular state value function.\"\"\"\n",
    "\n",
    "\tdef __init__(self, dimensions, learning_rate):\n",
    "\t\tself.table = np.zeros(dimensions)\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\t\"\"\"Returns the value of the specified state.\"\"\"\n",
    "\t\treturn self.table[state[0]][state[1]]\n",
    "\n",
    "\tdef step(self, state, error):\n",
    "\t\t\"\"\"Updates the value of the specified state.\"\"\"\n",
    "\t\tself.table[state[0]][state[1]] += self.learning_rate * error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new algorithm is only a slight modification of the previous one, and as such it makes sense to inherit much of the behaviour of that class. The resulting class adds additional code for storing values of states along each episode, and replaces the update function so that this is used to in updates for both the values and policies of states along the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
     "output_type": "error",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
      "at A.startServer (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:786120)",
      "at async A.ensureServerAndNotebookImpl (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:785575)"
     ]
    }
   ],
   "source": [
    "class monte_carlo_value_baseline(monte_carlo_returns):\n",
    "\t\"\"\"Contrasts returns with estimated values for policy updates.\"\"\"\n",
    "\n",
    "\tdef __init__(self, parameters):\n",
    "\t\tsuper().__init__(parameters)\n",
    "\t\tself.values = parameters['values']\n",
    "\t\tself.state_values = []\n",
    "\n",
    "\tdef _per_step(self):\n",
    "\t\t\"\"\"Adds required data storage for learning post-episode.\"\"\"\n",
    "\t\tsuper()._per_step()\n",
    "\t\tself.state_values.append(self.values.forward(self.past_state))\n",
    "\n",
    "\tdef _update(self):\n",
    "\t\t\"\"\"Loops over the episode in reverse, updating state policies and values.\"\"\"\n",
    "\t\tself.rewards = np.array(self.rewards)\n",
    "\t\tstate_return = 0\n",
    "\t\tfor index in range(len(self.states) - 1, -1, -1):\n",
    "\t\t\tstate_return += self.rewards[index]\n",
    "\t\t\terror = state_return - self.state_values[index]\n",
    "\t\t\tself.policy.step(self.states[index], error, self.eligibilities[index])\n",
    "\t\t\tself.values.step(self.states[index], error)\n",
    "\n",
    "\tdef _per_episode(self):\n",
    "\t\t\"\"\"Adds additional resets relevant to learning algorithm.\"\"\"\n",
    "\t\tsuper()._per_episode()\n",
    "\t\tself.state_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, we again initialize the environment, tables, and two agents using our two current algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
     "output_type": "error",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
      "at A.startServer (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:786120)",
      "at async A.ensureServerAndNotebookImpl (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:785575)"
     ]
    }
   ],
   "source": [
    "environment_parameters = dict(\n",
    "\ttrajectory_length = 20, \n",
    "\tpositivity_bias = 1,\n",
    "\ttarget_bias = 2,\n",
    ")\n",
    "environment = excursions(environment_parameters)\n",
    "\n",
    "table_dimension = (environment_parameters['trajectory_length']*2 + 1, \n",
    "\t\t\t\t   environment_parameters['trajectory_length'] + 1)\n",
    "policy1 = two_action_policy_table(table_dimension, 0.15)\n",
    "values2 = value_table(table_dimension, 0.6)\n",
    "policy2 = two_action_policy_table(table_dimension, 0.15)\n",
    "\n",
    "algorithm_parameters1 = dict(\n",
    "\tenvironment = environment, \n",
    "\treturn_learning_rate = 0.1,\n",
    "\tpolicy = policy1,\n",
    ")\n",
    "algorithm_parameters2 = dict(\n",
    "\tenvironment = environment, \n",
    "\treturn_learning_rate = 0.1,\n",
    "\tvalues = values2,\n",
    "\tpolicy = policy2,\n",
    ")\n",
    "agent1 = monte_carlo_returns(algorithm_parameters1)\n",
    "agent2 = monte_carlo_value_baseline(algorithm_parameters2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we again evaluate the initial policy and store samples for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
     "output_type": "error",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
      "at A.startServer (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:786120)",
      "at async A.ensureServerAndNotebookImpl (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:785575)"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "initial_return = agent1.evaluate(1000)\n",
    "agent2.average_return = initial_return\n",
    "print(\"Initial return: %s\"%(initial_return))\n",
    "initial_samples = agent1.samples(30)\n",
    "\n",
    "min_y = np.min(np.array(initial_samples)[:,:,0]) - 1\n",
    "max_y = np.max(np.array(initial_samples)[:,:,0]) + 1\n",
    "\n",
    "plt.plot(np.array(initial_samples)[:,:,0].T, c = 'k', alpha = 0.2)\n",
    "plt.scatter([20], [0], c = 'k', marker = 'o', s = 100)\n",
    "plt.plot([-1, 21], [0, 0], lw = 2, c = 'r', ls = '--', alpha = 0.3)\n",
    "plt.fill_between([-1, 21], [0, 0], [min_y, min_y], color = 'r', alpha = 0.1)\n",
    "plt.xlim(-1, 21)\n",
    "plt.ylim(min_y, max_y)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train both agents using their respective algorithms, plotting their returns against eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
     "output_type": "error",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
      "at A.startServer (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:786120)",
      "at async A.ensureServerAndNotebookImpl (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:785575)"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "agent1.train(episodes)\n",
    "agent2.train(episodes)\n",
    "\n",
    "plt.figure(figsize = (10, 3.5))\n",
    "plt.subplot(121)\n",
    "plt.plot(agent1.returns, c = 'b')\n",
    "plt.plot(agent2.returns, c = 'm')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Episodic returns\")\n",
    "plt.subplot(122)\n",
    "plt.plot(agent1.average_returns, c = 'b')\n",
    "plt.plot(agent2.average_returns, c = 'm')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Running return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare both of these with the original policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
     "output_type": "error",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.8.1 64-bit ('gym': conda).",
      "at A.startServer (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:786120)",
      "at async A.ensureServerAndNotebookImpl (c:\\Users\\domin\\.vscode\\extensions\\ms-python.python-2020.2.64397\\out\\client\\extension.js:1:785575)"
     ]
    }
   ],
   "source": [
    "final_return1 = agent1.evaluate(1000)\n",
    "final_return2 = agent2.evaluate(1000)\n",
    "print(\"Initial return: %s, agent1's final return: %s, agent2's final return: %s\"\n",
    "%(initial_return, final_return1, final_return2))\n",
    "samples1 = agent1.samples(30)\n",
    "samples2 = agent2.samples(30)\n",
    "\n",
    "plt.figure(figsize = (8, 3.5))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.array(initial_samples)[:,:,0].T, c = 'k', alpha = 0.2)\n",
    "plt.plot(np.array(samples1)[:,:,0].T, c = 'b', alpha = 0.2)\n",
    "plt.scatter([20], [0], c = 'k', marker = 'o', s = 80)\n",
    "plt.plot([-1, 21], [0, 0], lw = 2, c = 'r', ls = '--', alpha = 0.3)\n",
    "plt.fill_between([-1, 21], [0, 0], [min_y, min_y], color = 'r', alpha = 0.1)\n",
    "plt.xlim(-1, 21)\n",
    "plt.ylim(min_y, max_y)\n",
    "plt.title(\"Agent 1\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.subplot(122)\n",
    "plt.plot(np.array(initial_samples)[:,:,0].T, c = 'k', alpha = 0.2)\n",
    "plt.plot(np.array(samples2)[:,:,0].T, c = 'm', alpha = 0.2)\n",
    "plt.scatter([20], [0], c = 'k', marker = 'o', s = 80)\n",
    "plt.plot([-1, 21], [0, 0], lw = 2, c = 'r', ls = '--', alpha = 0.3)\n",
    "plt.fill_between([-1, 21], [0, 0], [min_y, min_y], color = 'r', alpha = 0.1)\n",
    "plt.xlim(-1, 21)\n",
    "plt.ylim(min_y, max_y)\n",
    "plt.title(\"Agent 2\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}