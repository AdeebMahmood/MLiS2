{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOz7N/sqKf7ryuKPvnbwosj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/examples/rnn/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alq1W_bCWoTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhl7G_SQ7wlH",
        "colab_type": "text"
      },
      "source": [
        "Download NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRyc8euxWpzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBvLWBQt8EZ6",
        "colab_type": "text"
      },
      "source": [
        "Upload deep_learning_sentences.txt file (or another file containing a list of sentences if you wish)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfpGBFRj-DzH",
        "colab_type": "code",
        "outputId": "ce6a9456-8a0e-4ed5-e43f-7bf613b394d1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3ac82561-3b02-4465-ba2c-35231e8806a2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3ac82561-3b02-4465-ba2c-35231e8806a2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving deep_learning_sentences.txt to deep_learning_sentences (1).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHNYmTqB93G9",
        "colab_type": "text"
      },
      "source": [
        "Add sentence start and end tags, convert to lower case and strip newlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyBM3rWv9chG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8QVY1IkAYEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('deep_learning_sentences.txt', 'r') as f:\n",
        "  sentences = f.readlines()\n",
        "sentences = [\"%s %s %s\" % (sentence_start_token, x.lstrip().rstrip('.\\n').lower(), sentence_end_token) for x in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPIWW5lqtSLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "989171e4-2bd2-48ed-cea0-77048b79c950"
      },
      "source": [
        "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
        "for i in range(0, 10):\n",
        "  print(\"Example: %s\" % sentences[i])"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsed 7674 sentences.\n",
            "Example: SENTENCE_START part ii  deep networks: modern  practices  166    this part of the book summarizes the state of modern deep learning as it is used to solve practical applications SENTENCE_END\n",
            "Example: SENTENCE_START this part focuses only on those approaches that are essentially working tech- nologies that are already used heavily in industry SENTENCE_END\n",
            "Example: SENTENCE_START by adding more layers and more units within a layer, a deep network can represent functions of increasing complexity SENTENCE_END\n",
            "Example: SENTENCE_START most tasks that consist of mapping an input vector to an output vector, and that are easy for a person to do rapidly, can be accomplished via deep learning, given sufficiently large models and sufficiently large datasets of labeled training examples SENTENCE_END\n",
            "Example: SENTENCE_START other tasks, that can not be described as associating one vector to another, or that are difficult enough that a person would require time to think and reflect in order to accomplish the task, remain beyond the scope of deep learning for now SENTENCE_END\n",
            "Example: SENTENCE_START this part of the book describes the core parametric function approximation technology that is behind nearly all modern practical applications of deep learning SENTENCE_END\n",
            "Example: SENTENCE_START scaling these models to large inputs such as high resolution images or long temporal sequences requires specialization SENTENCE_END\n",
            "Example: SENTENCE_START we introduce the convolutional network for scaling to large images and the recurrent neural network for processing temporal sequences SENTENCE_END\n",
            "Example: SENTENCE_START finally, we present general guidelines for the practical methodology involved in designing, building, and configuring an application involving deep learning, and review some of the applications of deep learning SENTENCE_END\n",
            "Example: SENTENCE_START these chapters are the most important for a practitioner—someone who wants to begin implementing and using deep learning algorithms to solve real-world  problems today SENTENCE_END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTBDO_fs-udT",
        "colab_type": "text"
      },
      "source": [
        "Tokenize the sentences into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRfoIQZV-tNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDJ1NGJy-7i_",
        "colab_type": "code",
        "outputId": "11266b04-8a24-4daf-b27b-4309afd8c629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13509 unique words tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGpHm2s_IyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 1000\n",
        "unknown_token = 'UNKNOWN_TOKEN'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBTJmo8G_Fv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = word_freq.most_common(vocab_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i, w in enumerate(index_to_word)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-C-wm5h_kQB",
        "colab_type": "text"
      },
      "source": [
        "Replace all words not in our vocabulary with the unknown token and discard sentences under min / over max number of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS0xWviykoTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_sentence_length = 10\n",
        "max_sentence_length = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXT2EKyw_h6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purged_sentences = []\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "  if min_sentence_length <= len(sent) <= max_sentence_length:\n",
        "    purged_sentences.append([w if w in word_to_index else unknown_token for w in sent])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r09ZMRxqk7pC",
        "colab_type": "code",
        "outputId": "bc1af432-58d6-43d6-d8d7-fbf4cd75df10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "print(\"Purged %d sentences.\" % (len(purged_sentences)))\n",
        "for i in range(0, 10):\n",
        "  print(\"Example: %s\" % purged_sentences[i])"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Purged 6791 sentences.\n",
            "Example: ['SENTENCE_START', 'part', 'UNKNOWN_TOKEN', 'deep', 'networks', ':', 'modern', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'this', 'part', 'of', 'the', 'book', 'UNKNOWN_TOKEN', 'the', 'state', 'of', 'modern', 'deep', 'learning', 'as', 'it', 'is', 'used', 'to', 'solve', 'practical', 'applications', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'this', 'part', 'UNKNOWN_TOKEN', 'only', 'on', 'those', 'approaches', 'that', 'are', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'that', 'are', 'already', 'used', 'UNKNOWN_TOKEN', 'in', 'UNKNOWN_TOKEN', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'by', 'adding', 'more', 'layers', 'and', 'more', 'units', 'within', 'a', 'layer', ',', 'a', 'deep', 'network', 'can', 'represent', 'functions', 'of', 'increasing', 'UNKNOWN_TOKEN', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'most', 'tasks', 'that', 'UNKNOWN_TOKEN', 'of', 'mapping', 'an', 'input', 'vector', 'to', 'an', 'output', 'vector', ',', 'and', 'that', 'are', 'easy', 'for', 'a', 'UNKNOWN_TOKEN', 'to', 'do', 'UNKNOWN_TOKEN', ',', 'can', 'be', 'UNKNOWN_TOKEN', 'via', 'deep', 'learning', ',', 'given', 'UNKNOWN_TOKEN', 'large', 'models', 'and', 'UNKNOWN_TOKEN', 'large', 'datasets', 'of', 'labeled', 'training', 'examples', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'other', 'tasks', ',', 'that', 'can', 'not', 'be', 'described', 'as', 'UNKNOWN_TOKEN', 'one', 'vector', 'to', 'another', ',', 'or', 'that', 'are', 'difficult', 'enough', 'that', 'a', 'UNKNOWN_TOKEN', 'would', 'require', 'time', 'to', 'think', 'and', 'UNKNOWN_TOKEN', 'in', 'order', 'to', 'UNKNOWN_TOKEN', 'the', 'task', ',', 'remain', 'UNKNOWN_TOKEN', 'the', 'UNKNOWN_TOKEN', 'of', 'deep', 'learning', 'for', 'now', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'this', 'part', 'of', 'the', 'book', 'UNKNOWN_TOKEN', 'the', 'UNKNOWN_TOKEN', 'parametric', 'function', 'approximation', 'UNKNOWN_TOKEN', 'that', 'is', 'UNKNOWN_TOKEN', 'nearly', 'all', 'modern', 'practical', 'applications', 'of', 'deep', 'learning', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'scaling', 'these', 'models', 'to', 'large', 'inputs', 'such', 'as', 'high', 'UNKNOWN_TOKEN', 'images', 'or', 'long', 'UNKNOWN_TOKEN', 'sequences', 'requires', 'UNKNOWN_TOKEN', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'we', 'introduce', 'the', 'convolutional', 'network', 'for', 'scaling', 'to', 'large', 'images', 'and', 'the', 'recurrent', 'neural', 'network', 'for', 'processing', 'UNKNOWN_TOKEN', 'sequences', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'UNKNOWN_TOKEN', ',', 'we', 'present', 'general', 'UNKNOWN_TOKEN', 'for', 'the', 'practical', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'in', 'UNKNOWN_TOKEN', ',', 'UNKNOWN_TOKEN', ',', 'and', 'UNKNOWN_TOKEN', 'an', 'application', 'UNKNOWN_TOKEN', 'deep', 'learning', ',', 'and', 'UNKNOWN_TOKEN', 'some', 'of', 'the', 'applications', 'of', 'deep', 'learning', 'SENTENCE_END']\n",
            "Example: ['SENTENCE_START', 'these', 'UNKNOWN_TOKEN', 'are', 'the', 'most', 'important', 'for', 'a', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'to', 'begin', 'UNKNOWN_TOKEN', 'and', 'using', 'deep', 'learning', 'algorithms', 'to', 'solve', 'UNKNOWN_TOKEN', 'problems', 'today', 'SENTENCE_END']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx-hXzf4_8g1",
        "colab_type": "text"
      },
      "source": [
        "Create the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnTX5tex_zg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in purged_sentences])\n",
        "Y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in purged_sentences])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTtRTv3PjLn0",
        "colab_type": "text"
      },
      "source": [
        "Limit to 100 training examples to save time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dMYm3Y3jBmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train[0:100]\n",
        "Y_train = Y_train[0:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUwaGdwHIBtc",
        "colab_type": "code",
        "outputId": "7cf46d59-43d7-4853-eacb-339f180c6307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Example: \", X_train[2])"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example:  [2, 22, 547, 66, 172, 11, 66, 78, 451, 8, 119, 1, 8, 43, 47, 18, 307, 127, 4, 753, 999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpzQxszlDOYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    xt = np.exp(x - np.max(x))\n",
        "    return xt / np.sum(xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yr2h159_7F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN:\n",
        "    \n",
        "  def __init__(self, word_dim, hidden_dim=100):\n",
        "      # Assign instance variables\n",
        "      self.word_dim = word_dim\n",
        "      self.hidden_dim = hidden_dim\n",
        "      # Randomly initialize the network parameters\n",
        "      self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (word_dim, hidden_dim))\n",
        "      self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, word_dim))\n",
        "      self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "      self.b = np.zeros(hidden_dim)\n",
        "      self.c = np.zeros(word_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Do a forward pass for single example\n",
        "    T = len(x)\n",
        "    h = np.zeros((T , self.hidden_dim))\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    for t in range(T):\n",
        "      # Note that we are indexing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
        "      if t == 1:\n",
        "        h[t] = self.U[x[t], :] + self.b\n",
        "      else:\n",
        "        h[t] = np.matmul(self.W.T, h[t-1]) + self.U[x[t], :] + self.b\n",
        "      o[t] = softmax(np.matmul(self.V.T, h[t]) + self.c)\n",
        "    return (o, h)\n",
        "\n",
        "  def backward(self, x, y):\n",
        "    #Do a backward pass for single example\n",
        "    T = len(x)\n",
        "    o, h = self.forward(x)\n",
        "    # We accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    dLdb = np.zeros(self.b.shape)\n",
        "    dLdc = np.zeros(self.c.shape)\n",
        "    # dL/do\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1.\n",
        "    # dL/dh\n",
        "    delta_h = np.zeros((T, self.hidden_dim))\n",
        "    for t in reversed(range(T)):\n",
        "      if t == T - 1:\n",
        "        delta_h[t] = np.matmul(self.V, delta_o[t, :])\n",
        "      else:\n",
        "        delta_h[t] = np.matmul(np.matmul(self.W, np.diag(1 - h[t+1]**2)), h[t+1]) + np.matmul(self.V, delta_o[t, :])\n",
        "    return (dLdU, dLdV, dLdW, dLdb, dLdc)\n",
        "\n",
        "  def step(self, x, y, learning_rate=0.1):\n",
        "    # Perform SGD step for single example\n",
        "    dLdU, dLdV, dLdW, dLdb, dLdc  = self.backward(x, y)\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "    self.b -= learning_rate * dLdb\n",
        "    self.c -= learning_rate * dLdc\n",
        "\n",
        "  def loss(self, x, y):\n",
        "    # Per example loss\n",
        "    o, h = self.forward(x)\n",
        "    return - np.sum(o[np.arange(len(y)), y])\n",
        "\n",
        "  def generate_sentence(self, max_length=20):\n",
        "    # We start the sentence with the start token\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    # Repeat until we get an end token or reach maximum sentence length\n",
        "    while not new_sentence[-1] == word_to_index[sentence_end_token] and len(new_sentence) < max_length:\n",
        "      o, h = self.forward(new_sentence)\n",
        "      sampled_word = word_to_index[unknown_token]\n",
        "      # We don't want to sample unknown words or sentence start\n",
        "      while sampled_word == word_to_index[unknown_token] or sampled_word == word_to_index[sentence_start_token]:\n",
        "          samples = np.random.multinomial(1, o[-1])\n",
        "          sampled_word = np.argmax(samples)\n",
        "      new_sentence.append(sampled_word)\n",
        "    sentence_str = [index_to_word[x] for x in new_sentence]\n",
        "    return sentence_str\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRw6_OG4CNt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(vocab_size)\n",
        "model.step(X_train[0], Y_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axCMZHGv2mae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbYNOpEQM7sI",
        "colab_type": "text"
      },
      "source": [
        "Generate random sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UI5anlSCSxe",
        "colab_type": "code",
        "outputId": "103a4f02-d97f-4aed-c382-703c17f76cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(model.generate_sentence())"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SENTENCE_START', 'between', ']', 'approximately', 'discrete', 'early', 'efficiently', 'across', 'update', 'while', 'squared', 'estimates', 'groups', 'position', 'less', 'connected', 'implementations', 'rule', 'datasets', '\\ue011']\n",
            "['SENTENCE_START', 'equations', 'refer', 'rn', 'algorithms', 'which', 'pca', 'pca', 'concepts', 'observe', 'divergence', 'based', 'graphical', 'architectures', 'choice', 'uses', 'adding', 'move', 'that', 'running']\n",
            "['SENTENCE_START', 'x|', 'component', 'found', 'average', 'per', 'language', 'markov', 'noise', 'posterior', 'generalize', 'shows', '\\ue011', 'define', 'sampled', 'pages', 'typical', 'h|', 'represent', 'right']\n",
            "['SENTENCE_START', 'vectors', 'problem', 'spatial', 'techniques', '2006', '“', 'numbers', '\\ue012', 'over', 'of', 'roughly', 'though', 'min', 'solve', 'sequences', 'practical', ')', 'processing', 'left']\n",
            "['SENTENCE_START', 'd', 'hyperparameters', 'boltzmann', 'without', 'computer', 'probability', 'β', 'nets', 'properties', 'β', 'within', 'new', 'kind', 'bengio', 'b', 'though', 'outputs', 'markov', 'sign']\n",
            "['SENTENCE_START', 'specific', 'p̃', 'similar', 'hessian', 'unfortunately', 'observed', 'starting', 'optimization', 'v.', 'minima', 'o', 'generated', 'bound', 'current', 'weight', 'human', 'trained', 'throughout', 'each']\n",
            "['SENTENCE_START', 'max', 'variation', 'bias', '\\ue03e', 'that', 'carlo', 'estimation', 'does', 'pixel', 'τ', 'early', 'concept', 'avoid', 'illustrated', 'variables', 'cells', '0', 'activation', 'approaches']\n",
            "['SENTENCE_START', 'work', 'overfitting', '”', 'multiplication', 'that', 'they', 'u', 'number', 'noise', 'process', 'advances', 'is', 'can', 'modern', 'parameter', 'challenge', 'likelihood', 'and', 'yield']\n",
            "['SENTENCE_START', 'convolution', 'estimates', 'wish', 'restricted', 'contains', 'regularized', 'require', 'difference', 'introduced', 'new', 'steps', 'input', 'properties', 'kinds', 'minimizing', 'provides', 'usually', 'left', 'expectation']\n",
            "['SENTENCE_START', '||', 'operation', 'data', 'through', 'few', 'field', 'rectified', 'strategies', 'sequence', 'generalize', 'new', 'groups', 'estimating', 'o', 'wide', 'were', 'they', 'x|', 'p']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoqDgdZLfFyf",
        "colab_type": "code",
        "outputId": "1141dacb-711e-4dab-f615-347869726581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total_loss = 0\n",
        "for i in range(len(X_train)):\n",
        "  total_loss += model.loss(X_train[i], Y_train[i])\n",
        "total_loss = total_loss / len(X_train)\n",
        "print(\"Loss \", total_loss)"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss  -0.02784692472348057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAjHknvCb7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 1\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcVe2NKPZlk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_history = []\n",
        "for epoch in range(num_epochs):\n",
        "  for i in range(len(X_train)):\n",
        "    model.step(X_train[i], Y_train[i])\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_wr2sH3bBss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lGGVmE3hkMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}