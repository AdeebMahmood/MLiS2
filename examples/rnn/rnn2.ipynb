{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMQwHCa04FQHaqpssGRDNZ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/examples/rnn/rnn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alq1W_bCWoTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzJs4xJfq6-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhl7G_SQ7wlH",
        "colab_type": "text"
      },
      "source": [
        "Download NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRyc8euxWpzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBvLWBQt8EZ6",
        "colab_type": "text"
      },
      "source": [
        "Upload deep_learning_sentences.txt file (or another file containing a list of sentences if you wish)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfpGBFRj-DzH",
        "colab_type": "code",
        "outputId": "b13087d4-ab6d-49fe-e53a-aa856e01e003",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-76425a87-458c-4ba9-8914-48270bbebde6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-76425a87-458c-4ba9-8914-48270bbebde6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving deep_learning_sentences.txt to deep_learning_sentences.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHNYmTqB93G9",
        "colab_type": "text"
      },
      "source": [
        "Add sentence start and end tags, convert to lower case and strip newlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyBM3rWv9chG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8QVY1IkAYEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('deep_learning_sentences.txt', 'r') as f:\n",
        "  sentences = f.readlines()\n",
        "sentences = [\"%s %s %s\" % (sentence_start_token, x.lstrip().rstrip('.\\n').lower(), sentence_end_token) for x in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPIWW5lqtSLG",
        "colab_type": "code",
        "outputId": "d0e31864-7a1a-453e-f8a4-d190420a1edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
        "for i in range(0, 10):\n",
        "  print(\"Example: %s\" % sentences[i])"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsed 7674 sentences.\n",
            "Example: SENTENCE_START part ii  deep networks: modern  practices  166    this part of the book summarizes the state of modern deep learning as it is used to solve practical applications SENTENCE_END\n",
            "Example: SENTENCE_START this part focuses only on those approaches that are essentially working tech- nologies that are already used heavily in industry SENTENCE_END\n",
            "Example: SENTENCE_START by adding more layers and more units within a layer, a deep network can represent functions of increasing complexity SENTENCE_END\n",
            "Example: SENTENCE_START most tasks that consist of mapping an input vector to an output vector, and that are easy for a person to do rapidly, can be accomplished via deep learning, given sufficiently large models and sufficiently large datasets of labeled training examples SENTENCE_END\n",
            "Example: SENTENCE_START other tasks, that can not be described as associating one vector to another, or that are difficult enough that a person would require time to think and reflect in order to accomplish the task, remain beyond the scope of deep learning for now SENTENCE_END\n",
            "Example: SENTENCE_START this part of the book describes the core parametric function approximation technology that is behind nearly all modern practical applications of deep learning SENTENCE_END\n",
            "Example: SENTENCE_START scaling these models to large inputs such as high resolution images or long temporal sequences requires specialization SENTENCE_END\n",
            "Example: SENTENCE_START we introduce the convolutional network for scaling to large images and the recurrent neural network for processing temporal sequences SENTENCE_END\n",
            "Example: SENTENCE_START finally, we present general guidelines for the practical methodology involved in designing, building, and configuring an application involving deep learning, and review some of the applications of deep learning SENTENCE_END\n",
            "Example: SENTENCE_START these chapters are the most important for a practitioner—someone who wants to begin implementing and using deep learning algorithms to solve real-world  problems today SENTENCE_END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTBDO_fs-udT",
        "colab_type": "text"
      },
      "source": [
        "Tokenize the sentences into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRfoIQZV-tNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDJ1NGJy-7i_",
        "colab_type": "code",
        "outputId": "a9018dc1-8238-49c2-d324-ebd56fb722e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13509 unique words tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGpHm2s_IyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 1000\n",
        "unknown_token = 'UNKNOWN_TOKEN'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBTJmo8G_Fv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = word_freq.most_common(vocab_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i, w in enumerate(index_to_word)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-C-wm5h_kQB",
        "colab_type": "text"
      },
      "source": [
        "Replace all words not in our vocabulary with the unknown token and discard sentences under min / over max number of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS0xWviykoTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sentence_length = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXT2EKyw_h6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purged_sentences = []\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "    purged_sentences.append([w if w in word_to_index else unknown_token for w in sent[0:max_sentence_length]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r09ZMRxqk7pC",
        "colab_type": "code",
        "outputId": "67812f27-4684-4c58-b6b6-b6e279c6f161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(\"Purged %d sentences.\" % (len(purged_sentences)))\n",
        "for i in range(0, 10):\n",
        "  print(\"Example: %s\" % purged_sentences[i])"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Purged 7674 sentences.\n",
            "Example: ['SENTENCE_START', 'part', 'UNKNOWN_TOKEN', 'deep', 'networks']\n",
            "Example: ['SENTENCE_START', 'this', 'part', 'UNKNOWN_TOKEN', 'only']\n",
            "Example: ['SENTENCE_START', 'by', 'adding', 'more', 'layers']\n",
            "Example: ['SENTENCE_START', 'most', 'tasks', 'that', 'UNKNOWN_TOKEN']\n",
            "Example: ['SENTENCE_START', 'other', 'tasks', ',', 'that']\n",
            "Example: ['SENTENCE_START', 'this', 'part', 'of', 'the']\n",
            "Example: ['SENTENCE_START', 'scaling', 'these', 'models', 'to']\n",
            "Example: ['SENTENCE_START', 'we', 'introduce', 'the', 'convolutional']\n",
            "Example: ['SENTENCE_START', 'UNKNOWN_TOKEN', ',', 'we', 'present']\n",
            "Example: ['SENTENCE_START', 'these', 'UNKNOWN_TOKEN', 'are', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx-hXzf4_8g1",
        "colab_type": "text"
      },
      "source": [
        "Create the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnTX5tex_zg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in purged_sentences])\n",
        "Y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in purged_sentences])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUwaGdwHIBtc",
        "colab_type": "code",
        "outputId": "d2172ca0-a439-401c-8ba7-7eecaca9c025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Example: \", X_train[2])"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example:  [  2  22 547  66]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpzQxszlDOYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    xt = np.exp(x - np.max(x))\n",
        "    return xt / np.sum(xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yr2h159_7F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN:\n",
        "    \n",
        "  def __init__(self, word_dim, hidden_dim=100):\n",
        "      # Assign instance variables\n",
        "      self.word_dim = word_dim\n",
        "      self.hidden_dim = hidden_dim\n",
        "      # Randomly initialize the network parameters\n",
        "      self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "      self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "      self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "      self.b = np.zeros(hidden_dim)\n",
        "      self.c = np.zeros(word_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Do a forward pass for single example\n",
        "    T = len(x)\n",
        "    h = np.zeros((T , self.hidden_dim))\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    for t in range(T):\n",
        "      # Note that we are indexing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
        "      h[t] = self.U[:, x[t]] + self.b\n",
        "      if t > 1:\n",
        "        h[t] += np.matmul(self.W, h[t-1])\n",
        "      o[t] = softmax(np.matmul(self.V, h[t]) + self.c)\n",
        "    return (o, h)\n",
        "\n",
        "  def backward(self, x, y, clip_value=None):\n",
        "    #Do a backward pass for single example\n",
        "    T = len(x)\n",
        "    o, h = self.forward(x)\n",
        "    # Accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    dLdb = np.zeros(self.b.shape)\n",
        "    dLdc = np.zeros(self.c.shape)\n",
        "    # dL/do\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1.\n",
        "    # dL/dh\n",
        "    delta_h = np.zeros((T, self.hidden_dim))\n",
        "    for t in reversed(range(T)):\n",
        "      delta_h[t] = np.matmul(self.V.T, delta_o[t, :])\n",
        "      if t < T - 1:\n",
        "        delta_h[t] += np.matmul(np.matmul(self.W.T, delta_h[t+1]), np.diag(1 - h[t+1]**2))\n",
        "    # Accumulate gradients over time-steps\n",
        "    for t in range(T):\n",
        "      dLdc += delta_o[t, :]\n",
        "      dLdb += (1 - h[t]**2) * delta_h[t, :]\n",
        "      dLdV += np.outer(delta_o[t, :], h[t, :])\n",
        "      if t > 0:\n",
        "        dLdW += np.matmul(np.diag(1 - h[t]**2), np.outer(delta_h[t, :], h[t-1, :]))\n",
        "      xm = np.zeros((self.word_dim))\n",
        "      xm[x] = 1\n",
        "      dLdU += np.matmul(np.diag(1 - h[t]**2), np.outer(delta_h[t, :], xm))\n",
        "    if clip_value is not None:\n",
        "      dLdb = np.clip(dLdb, -clip_value, clip_value)\n",
        "      dLdc = np.clip(dLdc, -clip_value, clip_value)\n",
        "      dLdV = np.clip(dLdV, -clip_value, clip_value)\n",
        "      dLdW = np.clip(dLdW, -clip_value, clip_value)\n",
        "      dLdU = np.clip(dLdU, -clip_value, clip_value)\n",
        "    return (dLdU, dLdV, dLdW, dLdb, dLdc)\n",
        "\n",
        "  def step(self, x, y, learning_rate=0.01):\n",
        "    # Perform SGD step for single example\n",
        "    dLdU, dLdV, dLdW, dLdb, dLdc  = self.backward(x, y)\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "    self.b -= learning_rate * dLdb\n",
        "    self.c -= learning_rate * dLdc\n",
        "\n",
        "  def loss(self, x, y):\n",
        "    # Per example loss\n",
        "    o, h = self.forward(x)\n",
        "    return - np.sum(o[np.arange(len(y)), y])\n",
        "\n",
        "  def generate_sentence(self, max_length=20):\n",
        "    # We start the sentence with the start token\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    # Repeat until we get an end token or reach maximum sentence length\n",
        "    while not new_sentence[-1] == word_to_index[sentence_end_token] and len(new_sentence) < max_length:\n",
        "      o, h = self.forward(new_sentence)\n",
        "      sampled_word = word_to_index[unknown_token]\n",
        "      # We don't want to sample unknown words or sentence start\n",
        "      while sampled_word == word_to_index[unknown_token] or sampled_word == word_to_index[sentence_start_token]:\n",
        "          samples = np.random.multinomial(1, o[-1])\n",
        "          sampled_word = np.argmax(samples)\n",
        "      new_sentence.append(sampled_word)\n",
        "    sentence_str = [index_to_word[x] for x in new_sentence]\n",
        "    return sentence_str\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRw6_OG4CNt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbYNOpEQM7sI",
        "colab_type": "text"
      },
      "source": [
        "Generate random sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UI5anlSCSxe",
        "colab_type": "code",
        "outputId": "5e73036a-4bc8-453d-c03b-37b3f166eccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(model.generate_sentence())"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SENTENCE_START', 'classifier', 'σ', 'al', 'd.', 'et', 'positive', 'move', 'unit', 'f', 'with', 'tasks', 'value', 'directions', 'approximation', 'see', 'exponential', 'use', 'exactly', 'coding']\n",
            "['SENTENCE_START', 'regularization', 'dimension', 'q', 'represent', 'cases', 'initialization', 'state', 'weight', 'both', 'yy', 'rate', '2006', 'highly', 'pooling', 'hidden', 'estimation', '[', 'logistic', 'expectation']\n",
            "['SENTENCE_START', 'write', 'should', 'involve', 'strong', 'systems', 'translation', 'itself', 'to', 'information', 'learning', 'size', 'discrete', 'tangent', '\\ue010', '∂u', 'image', 'eigenvalues', 'sum', 'recursive']\n",
            "['SENTENCE_START', 'softmax', 'help', 'no', 'straightforward', 'increase', 'field', 'any', 'd.', 'figure', 'numbers', 'each', 'minimum', 'containing', 'relevant', 'regularization', 'data', 'decay', 'code', 'representing']\n",
            "['SENTENCE_START', 'speech', 'solution', 'space', 'kernel', 'j', 'see', 'become', 'it', 'terms', 'generator', 'assume', 'observed', 'connected', 'especially', 'α', 'changes', 'additional', 'name', '2006']\n",
            "['SENTENCE_START', 'higher', 'equal', 'xi', 'batch', 'consists', 'structure', 'left', 'or', 'solve', 'refer', 'ŷ', '”', 'seen', 'nets', 'algorithm', 'consider', 'wide', 'therefore', 'high']\n",
            "['SENTENCE_START', 'vision', 'a.', 'visible', 'principle', 'several', 'λ', 'dataset', 'hinton', 'increasing', 'new', 'modes', 'requires', 'pages', 'equivalent', 'concepts', 'l.', 'important', 'softmax', 'score']\n",
            "['SENTENCE_START', 'h.', 'approximate', 'sum', 't', 'posterior', 'randomly', 'modes', 'found', 'becomes', 'boltzmann', 'necessary', 'approaches', 'yet', 'able', 'too', 'equation', 'variable', 'real', 'describing']\n",
            "['SENTENCE_START', 'within', 'computation', 'properties', 'typically', 'intelligence', 'yy', 'many', 'example', 'draw', 'parameter', 'corresponding', 'momentum', 'approaches', 'generalize', 'scaling', 'between', 'difference', 'would', 'via']\n",
            "['SENTENCE_START', 'require', 'maximizing', '\\ue030', 'many', 'yy', 'having', 'l', 'key', 'unsupervised', 'model', 'assumption', 't.', 'good', 'log-likelihood', 'object', 'parallel', 'on', '3', 'choosing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAjHknvCb7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 300\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuZmmxKOvjSh",
        "colab_type": "text"
      },
      "source": [
        "Limit training examples to save time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhOMk8V_viNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train[0:1000]\n",
        "Y_train = Y_train[0:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcVe2NKPZlk5",
        "colab_type": "code",
        "outputId": "ce812b6f-9c2e-47c7-c81c-45bbd714c0c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss_history = []\n",
        "for epoch in range(num_epochs):\n",
        "  loss = 0\n",
        "  for i in range(len(X_train)):\n",
        "    loss += model.loss(X_train[i], Y_train[i])\n",
        "  loss = loss / len(X_train)\n",
        "  print(\"Epoch {0} Loss {1}\".format(epoch , loss))\n",
        "  loss_history.append(loss)\n",
        "  for i in range(len(X_train)):\n",
        "    model.step(X_train[i], Y_train[i], learning_rate=learning_rate)\n",
        "    \n"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss -0.0039958404631192995\n",
            "Epoch 1 Loss -0.0040633058957869886\n",
            "Epoch 2 Loss -0.004133656686203513\n",
            "Epoch 3 Loss -0.004207997351465952\n",
            "Epoch 4 Loss -0.00428767191662882\n",
            "Epoch 5 Loss -0.004374383200676639\n",
            "Epoch 6 Loss -0.004470370019959061\n",
            "Epoch 7 Loss -0.004578683159123945\n",
            "Epoch 8 Loss -0.004703635679403707\n",
            "Epoch 9 Loss -0.004851575438412534\n",
            "Epoch 10 Loss -0.0050322881791335095\n",
            "Epoch 11 Loss -0.005261724206624522\n",
            "Epoch 12 Loss -0.00556775335233724\n",
            "Epoch 13 Loss -0.006003631356717491\n",
            "Epoch 14 Loss -0.0066839116771677375\n",
            "Epoch 15 Loss -0.007897239698930116\n",
            "Epoch 16 Loss -0.010530454500112535\n",
            "Epoch 17 Loss -0.01773272145191047\n",
            "Epoch 18 Loss -0.03672239705463015\n",
            "Epoch 19 Loss -0.06293018709932753\n",
            "Epoch 20 Loss -0.08498634165383445\n",
            "Epoch 21 Loss -0.10645463741119103\n",
            "Epoch 22 Loss -0.09101243683515874\n",
            "Epoch 23 Loss -0.08401576546541206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in matmul\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py:905: RuntimeWarning: invalid value encountered in multiply\n",
            "  return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in matmul\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: overflow encountered in matmul\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: RuntimeWarning: overflow encountered in square\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24 Loss nan\n",
            "Epoch 25 Loss nan\n",
            "Epoch 26 Loss nan\n",
            "Epoch 27 Loss nan\n",
            "Epoch 28 Loss nan\n",
            "Epoch 29 Loss nan\n",
            "Epoch 30 Loss nan\n",
            "Epoch 31 Loss nan\n",
            "Epoch 32 Loss nan\n",
            "Epoch 33 Loss nan\n",
            "Epoch 34 Loss nan\n",
            "Epoch 35 Loss nan\n",
            "Epoch 36 Loss nan\n",
            "Epoch 37 Loss nan\n",
            "Epoch 38 Loss nan\n",
            "Epoch 39 Loss nan\n",
            "Epoch 40 Loss nan\n",
            "Epoch 41 Loss nan\n",
            "Epoch 42 Loss nan\n",
            "Epoch 43 Loss nan\n",
            "Epoch 44 Loss nan\n",
            "Epoch 45 Loss nan\n",
            "Epoch 46 Loss nan\n",
            "Epoch 47 Loss nan\n",
            "Epoch 48 Loss nan\n",
            "Epoch 49 Loss nan\n",
            "Epoch 50 Loss nan\n",
            "Epoch 51 Loss nan\n",
            "Epoch 52 Loss nan\n",
            "Epoch 53 Loss nan\n",
            "Epoch 54 Loss nan\n",
            "Epoch 55 Loss nan\n",
            "Epoch 56 Loss nan\n",
            "Epoch 57 Loss nan\n",
            "Epoch 58 Loss nan\n",
            "Epoch 59 Loss nan\n",
            "Epoch 60 Loss nan\n",
            "Epoch 61 Loss nan\n",
            "Epoch 62 Loss nan\n",
            "Epoch 63 Loss nan\n",
            "Epoch 64 Loss nan\n",
            "Epoch 65 Loss nan\n",
            "Epoch 66 Loss nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-243-3805ed178d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-238-bff5746a6d3e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x, y, learning_rate)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Perform SGD step for single example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mdLdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdLdU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdLdV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-238-bff5746a6d3e>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, y, clip_value)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mxm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mxm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0mdLdU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0mdLdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lGGVmE3hkMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "ax = plt.subplot(1, 1, 1)\n",
        "ax.plot(loss_history[:])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('Loss', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmpzCd6-KMru",
        "colab_type": "code",
        "outputId": "a93e09bd-38a2-4f66-a30b-fea36ed6032e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(model.generate_sentence())"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SENTENCE_START', 'sampling', '\\ue058', 'example', ',', 'f', 'must', 'software', '5', 'sampled', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n",
            "['SENTENCE_START', 'this', 'entries', 'we', 'have', '0', 'scalar', 'fixed', 'evaluating', 'sampled', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n",
            "['SENTENCE_START', 'however', '4', 'models', 'move', 'yy', 'word', 'procedure', 'evaluating', 'though', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n",
            "['SENTENCE_START', ',', 'denoising', 'to', 'provides', '2015', 'such', 't.', 'simpler', 'related', 'this', 'this', ',', 'of', 'therefore', 'therefore', 'assume', 'sampled', 'this', 'this']\n",
            "['SENTENCE_START', 'basics', 'estimates', 'network', 'data', '0', 'throughout', 'l', '2', 'likelihood', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n",
            "['SENTENCE_START', '(', 'second', 'pixels', 'have', 'must', 'convolutional', 'easier', '=', 'sampled', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n",
            "['SENTENCE_START', 'and', 'overfitting', 'x|', 'neural', 'the', 'back-propagation', 'variable', '=', 'not', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n",
            "['SENTENCE_START', 'gradient', 'c.', 'linear', 'now', 'gradient', 'dimension', 'decay', 'recognition', 'sampled', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n",
            "['SENTENCE_START', 'x', 'between', 'the', 'learning', ';', 'convolutional', 'entire', 'regression', 'challenge', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n",
            "['SENTENCE_START', 'this', 'approach', ',', 'function', 'contains', 'neurons', 'ways', 'mcmc', 'bound', 'this', 'this', ',', 'the', '0', 'therefore', 'assume', 'sampled', 'sampled', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D16DIBo1Kbep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}