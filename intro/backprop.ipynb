{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backprop.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSeIivBWlZBjjTBmJzEOb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/intro/backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64i7hjgm_Xqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq1AS2ym_fP0",
        "colab_type": "text"
      },
      "source": [
        "Let us again attempt to learn the XOR function using the same MLP network,  this time starting with random initial weights and using back-propogation with simple gradient descent.\n",
        "\n",
        "The error functions for each neuron are\n",
        "\\begin{eqnarray}\n",
        "\\Delta_1^{(2)} &=& {\\partial{J} \\over \\partial a_1^{(2)}}\\,, \\\\\n",
        "\\Delta_1^{(1)} &=&  \\Delta_1^{(2)} W_{11}^{(2)}   \\Theta ( z_1^{(1)} )  \\\\\n",
        "\\Delta_2^{(1)} &=&  \\Delta_1^{(2)} W_{21}^{(2)}  \\Theta ( z_2^{(1)} )  \\,,\n",
        "\\end{eqnarray}\n",
        "since the Heavisde step function $\\Theta$ is the derivative of the ReLU activation function."
      ]
    }
  ]
}