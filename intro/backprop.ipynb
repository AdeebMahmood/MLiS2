{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backprop.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiSRYPQcmrMDP1R4WE5VDg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/intro/backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq1AS2ym_fP0",
        "colab_type": "text"
      },
      "source": [
        "Let us again attempt to learn the XOR function using the same MLP network,  this time starting with random initial weights and using back-propogation with simple gradient descent.\n",
        "\n",
        "The error functions for each neuron are\n",
        "\\begin{eqnarray}\n",
        "\\Delta_1^{(2)} &=& {\\partial{J} \\over \\partial a_1^{(2)}}\\,, \\\\\n",
        "\\Delta_1^{(1)} &=&  \\Delta_1^{(2)} W_{11}^{(2)}   \\Theta ( z_1^{(1)} )  \\\\\n",
        "\\Delta_2^{(1)} &=&  \\Delta_1^{(2)} W_{21}^{(2)}  \\Theta ( z_2^{(1)} )  \\,,\n",
        "\\end{eqnarray}\n",
        "since the Heavisde step function $\\Theta$ is the derivative of the ReLU activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVbMGrw2__Zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73KKJzkHZq0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 100\n",
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_EC8JkUAHwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "X = np.hstack((np.ones(shape=(X.shape[0], 1)), X))\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzYShJU4ANmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = np.array([[0], [1], [1], [0]])\n",
        "print(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bab4ucpXARdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "W = np.random.random(size=(3,2)) - 0.5\n",
        "W = np.array([[0, -1], [1,1], [1,1]], dtype=np.float)\n",
        "print(W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHktKXhLAWx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = np.array([[0], [1], [-2]], dtype=np.float)\n",
        "w += 0.5 * (0.5 * np.random.random(size=(3,1)) - 0.5)\n",
        "#w[2] = -1.0\n",
        "print(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IP6GWSHP4Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  loss = 0\n",
        "  delta_2_1 = 0\n",
        "  delta_1_1 = 0\n",
        "  delta_1_2 = 0\n",
        "  for i in range(4):\n",
        "    x = X[i, :]\n",
        "    y = Y[i]\n",
        "    #print(x, y)\n",
        "    z_1 = np.matmul(x, W)\n",
        "    #print(z_1)\n",
        "    a_1 = np.maximum(z_1, 0)\n",
        "    #print(a_1)\n",
        "    a_1 = np.hstack((1, a_1))\n",
        "    #print(a_1)\n",
        "    yhat = np.matmul(a_1, w)\n",
        "    loss += 0.25 * (y - yhat)**2\n",
        "    #print(yhat)\n",
        "    delta = 0.5 * yhat * ( yhat - y)\n",
        "    delta_2_1 += delta\n",
        "    #print(delta_2)\n",
        "    delta_1_1 += delta * w[1, 0] * np.heaviside(z_1[0], 0) * a_1[1]\n",
        "    delta_1_2 += delta * w[2, 0] * np.heaviside(z_1[1], 0) * a_1[2]\n",
        "    #print(delta_2_1, delta_1_1, delta_1_2)\n",
        "  w[0, 0] -= learning_rate * delta_2_1\n",
        "  w[1, 0] -= learning_rate * delta_1_1 \n",
        "  w[2, 0] -= learning_rate * delta_1_2\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9MVI58YWMpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMnXBF5YePYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}