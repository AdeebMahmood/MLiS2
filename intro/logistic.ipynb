{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMCZIQwpC1zasgTwvdZY7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/intro/logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZAKI-4lIw1E",
        "colab_type": "text"
      },
      "source": [
        "The logistic model is given by \n",
        "$\n",
        "\\hat{y} = \\sigma \\left( \\mathbf{x}^T \\mathbf{w} \\right)\\,,\n",
        "$\n",
        "where $\\sigma$ is the sigmoid function. \n",
        "\n",
        "Logistic regression is commonly used to estimate the probability that an example belong to a specific binary class, i.e. \n",
        "$\n",
        "\\hat{y}  = P (y = 1 |  \\mathbf{x})\\,.\n",
        "$\n",
        "\n",
        "There is no general closed form solution for the weights, so we will use a similar iterative procedure to the perceptron algorithm. Since  the activation function is differentiable we can use gradient descent,  and if the loss function is convex it is guaranteed to find the optimal solution.  Gradient descent proposes a new set of weights\n",
        "\n",
        " $\n",
        " \\mathbf{w}  \\rightarrow  \\mathbf{w}  - \\alpha \\nabla_{\\mathbf{w}}  L  (\\mathbf{w})\\,, \n",
        " $\n",
        "\n",
        "where $\\alpha$ is the learning rate, a positive scalar that determines the size of the update. There are several ways to choose $\\alpha$, which we will discuss later on. For now we will set it to a small constant.  \n",
        "\n",
        "We will again attempt to learn the AND and XOR functions, but now use the BCE loss.  In the logistic model the gradient of the BCE loss with respect to the weights can be shown to be \n",
        "\n",
        " $\n",
        " \\nabla_{\\mathbf{w}}  L  (\\mathbf{w})  = \\frac{1}{N} \\sum^N_i \\left( \\hat{y}^{(i)}  - y^{(i)} \\right) \\mathbf{x}^{(i)} = \\frac{1}{N} \\mathbf{X}^T \\left( \\hat{\\mathbf{Y}} -  \\mathbf{Y}\\right)\\,,\n",
        " $\n",
        "\n",
        "Gradient descent for logistic regression therefore strongly resembles the perceptron learning algorithm. "
      ]
    }
  ]
}